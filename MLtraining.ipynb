{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc84e09-792f-436b-8f04-909ca5e866e1",
   "metadata": {},
   "source": [
    "# We will build our AI model in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3583007c-33df-407b-92d9-8b6c15ea91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187edf2-f25f-4623-bbbc-1cb24b13cb90",
   "metadata": {},
   "source": [
    "## Steps we will do in this notebook\n",
    "- Load Cleaned Dataset\n",
    "- Train Test Split\n",
    "- Define and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2fb69-4bca-4710-86c7-6d5ab3046f7d",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e54748-13fe-498e-84dc-78ef89faba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d63ac-8c01-4ae9-8267-4a694549db68",
   "metadata": {},
   "source": [
    "### Split features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ddad2b-ddc7-47e4-9d84-33cf003e5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c459ec-ca04-4f2b-9117-a2ec044219a8",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bf9638-62d6-4be8-97af-7728844d4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff6104-65ee-4bc6-84d4-8d5a211deb1b",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "- Linear Regression(as classifier)\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Then we will create a summary dictionary of all models and their accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31383c-b757-4db3-b231-c2b264a95035",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51156a07-a200-45d0-93e0-621ec6956678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Linear Regression as Classifier\n",
      "Accuracy: 0.9990510270654989\n",
      "AUC Score: 0.7499207936476504\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict & convert to binary\n",
    "linear_pred = np.where(linear_model.predict(x_test_scaled) > 0.5, 1, 0)\n",
    "linear_accuracy = accuracy_score(y_test, linear_pred)\n",
    "\n",
    "print(\"üîπ Linear Regression as Classifier\")\n",
    "print(\"Accuracy:\", linear_accuracy)\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, linear_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c512479-e162-41e9-ac0d-1fec150c0c91",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898f3526-4cf2-498a-b609-69f10321718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Logistic Regression\n",
      "Accuracy: 0.9993080405685929\n",
      "AUC Score: 0.8311707936476505\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "logistic_pred = logistic_model.predict(x_test_scaled)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_pred)\n",
    "\n",
    "print(\"üîπ Logistic Regression\")\n",
    "print(\"Accuracy:\", logistic_accuracy)\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, logistic_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbaae8-c527-4b09-a3ce-823284f456ee",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7045fdf9-4f70-470f-90ff-ac9e25765bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Decision Tree\n",
      "Accuracy: 0.9992091892212491\n",
      "AUC Score: 0.9060024801489079\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(x_train, y_train)\n",
    "\n",
    "tree_pred = tree_model.predict(x_test)\n",
    "tree_accuracy = accuracy_score(y_test, tree_pred)\n",
    "\n",
    "print(\"üîπ Decision Tree\")\n",
    "print(\"Accuracy:\", tree_accuracy)\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, tree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089a303-e760-43ce-9292-d23cf112594f",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9607cef1-b31f-4243-a156-076b97d73e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Random Forest\n",
      "Accuracy: 0.9996243648800933\n",
      "AUC Score: 0.9062103968238253\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(random_state=42)\n",
    "forest_model.fit(x_train, y_train)\n",
    "\n",
    "forest_pred = forest_model.predict(x_test)\n",
    "forest_accuracy = accuracy_score(y_test, forest_pred)\n",
    "\n",
    "print(\"üîπ Random Forest\")\n",
    "print(\"Accuracy:\", forest_accuracy)\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, forest_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05acb50b-2c82-4ac3-a1c7-746002106328",
   "metadata": {},
   "source": [
    "## Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c0326-1e90-4fae-8358-e3154a725cb2",
   "metadata": {},
   "source": [
    "#### Overall Random Forest is better model for this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2652f6-80d8-4392-9970-e55736343a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Tuning Logistic Regression...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "üìä Evaluation for Logistic Regression\n",
      "Best Params: {'clf__C': 10, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Accuracy: 99.9308%\n",
      "AUC Score: 0.9904365755133562\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     50501\n",
      "           1       0.87      0.66      0.75        80\n",
      "\n",
      "    accuracy                           1.00     50581\n",
      "   macro avg       0.93      0.83      0.88     50581\n",
      "weighted avg       1.00      1.00      1.00     50581\n",
      "\n",
      "\n",
      "üîç Tuning Random Forest...\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "\n",
      "üìä Evaluation for Random Forest\n",
      "Best Params: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Accuracy: 99.9644%\n",
      "AUC Score: 0.9680446921843131\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     50501\n",
      "           1       0.94      0.82      0.88        80\n",
      "\n",
      "    accuracy                           1.00     50581\n",
      "   macro avg       0.97      0.91      0.94     50581\n",
      "weighted avg       1.00      1.00      1.00     50581\n",
      "\n",
      "\n",
      "üîç Tuning Decision Tree...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "\n",
      "üìä Evaluation for Decision Tree\n",
      "Best Params: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Accuracy: 99.9506%\n",
      "AUC Score: 0.9444671887685391\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     50501\n",
      "           1       0.86      0.82      0.84        80\n",
      "\n",
      "    accuracy                           1.00     50581\n",
      "   macro avg       0.93      0.91      0.92     50581\n",
      "weighted avg       1.00      1.00      1.00     50581\n",
      "\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Logistic Regression: Accuracy=0.9993, F1=0.9993, AUC=0.9904\n",
      "Best Params: {'clf__C': 10, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "--------------------------------------------------\n",
      "Random Forest: Accuracy=0.9996, F1=0.9996, AUC=0.968\n",
      "Best Params: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "--------------------------------------------------\n",
      "Decision Tree: Accuracy=0.9995, F1=0.9995, AUC=0.9445\n",
      "Best Params: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ Best Model Selected: Random Forest\n",
      "‚úÖ Best model saved to 'best_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "model_dict = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Hyperparameter space\n",
    "search_space = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Models needing scaling\n",
    "scale_models = ['Logistic Regression']\n",
    "\n",
    "# Storage\n",
    "results = []\n",
    "model_store = {}\n",
    "\n",
    "# Filter hyperparams per model\n",
    "def filter_hyperparameter(model, space):\n",
    "    valid_keys = model.get_params().keys()\n",
    "    param_grid = {k: v for k, v in space.items() if k in valid_keys}\n",
    "    \n",
    "    model_name = type(model).__name__\n",
    "    if model_name == 'LogisticRegression' and 'penalty' in param_grid and 'solver' in param_grid:\n",
    "        param_combinations = []\n",
    "        for penalty in param_grid['penalty']:\n",
    "            for solver in param_grid['solver']:\n",
    "                if penalty == 'l1' and solver == 'lbfgs':\n",
    "                    continue\n",
    "                combo = {'penalty': [penalty], 'solver': [solver]}\n",
    "                for key in param_grid:\n",
    "                    if key not in combo:\n",
    "                        combo[key] = param_grid[key]\n",
    "                param_combinations.append(combo)\n",
    "        return param_combinations[0] if param_combinations else param_grid\n",
    "    return param_grid\n",
    "\n",
    "# Training loop\n",
    "for name, model in model_dict.items():\n",
    "    print(f\"\\nüîç Tuning {name}...\")\n",
    "\n",
    "    if name in scale_models:\n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "        model_for_grid = pipe\n",
    "        param_grid = {f'clf__{k}': v for k, v in filter_hyperparameter(model, search_space).items()}\n",
    "    else:\n",
    "        model_for_grid = model\n",
    "        param_grid = filter_hyperparameter(model, search_space)\n",
    "\n",
    "    cv_folds = 5 if name != 'Random Forest' else 3\n",
    "    n_jobs_param = -1 if name != 'Random Forest' else 8\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model_for_grid,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_folds,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=n_jobs_param,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(x_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    model_store[name] = best_model\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_proba = best_model.predict_proba(x_test)[:, 1]\n",
    "    elif hasattr(best_model, \"decision_function\"):\n",
    "        y_proba = best_model.decision_function(x_test)\n",
    "    else:\n",
    "        y_proba = None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else \"N/A\"\n",
    "\n",
    "    print(f\"\\nüìä Evaluation for {name}\")\n",
    "    print(\"Best Params:\", grid.best_params_)\n",
    "    print(\"Accuracy:\", f\"{acc*100:.4f}%\")\n",
    "    print(\"AUC Score:\", auc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': round(acc, 4),\n",
    "        'F1-Score': round(f1, 4),\n",
    "        'AUC': round(auc, 4) if y_proba is not None else \"N/A\",\n",
    "        'Best Params': grid.best_params_\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for r in results:\n",
    "    print(f\"{r['Model']}: Accuracy={r['Accuracy']}, F1={r['F1-Score']}, AUC={r['AUC']}\")\n",
    "    print(f\"Best Params: {r['Best Params']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Select and save best model (by F1-score)\n",
    "best_result = max(results, key=lambda x: x['F1-Score'])\n",
    "best_model_name = best_result['Model']\n",
    "print(f\"\\nüèÜ Best Model Selected: {best_model_name}\")\n",
    "best_model = model_store[best_model_name]\n",
    "\n",
    "# Save with pickle\n",
    "with open(\"best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(\"‚úÖ Best model saved to 'best_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
